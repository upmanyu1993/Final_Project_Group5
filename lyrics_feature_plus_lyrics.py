# -*- coding: utf-8 -*-
"""lyrics_feature_plus_lyrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18YEm1VPkq0MBSMI4QNkS-rdfLGmNF3hN
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras import layers
from keras.optimizers import Adam, SGD
from keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import plot_model
import plotly.express as px
from sklearn.decomposition import PCA

# # For working with google drive
# from google.colab import drive
# drive.mount('/content/gdrive')

from google.colab import drive
drive.mount('/content/drive')

# !pip3 install kaleido

# cd gdrive/MyDrive/Colab\ Notebooks/'Deep learning'/Spotify_Popularity_Predictor

"""## Auxiliary functions"""

def reduce_dimensionality(dataset, n_dimensions):
  """Reduces the size of the dataset to a given parameter"""
  pca = PCA(n_components=n_dimensions)
  return pca.fit_transform(dataset)

def generate_dataset(train_size, val_size):
  df = pd.read_csv('/content/drive/MyDrive/final_data.csv')
  df = df.dropna(subset=['track_popularity'])
  scaler = StandardScaler()
  y = df['track_popularity'].to_numpy()
  df.drop('track_popularity', axis = 1, inplace=True)
  x = scaler.fit_transform(df)
  x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = train_size, shuffle = True)
  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size = 1 - val_size, shuffle = True)
  return x_train, y_train, x_test, y_test, x_val, y_val

# df = pd.read_csv('/content/drive/MyDrive/final_data.csv')
# df.loc[df['track_popularity']<0.5, 'track_popularity'] = 0
# df.loc[df['track_popularity']>=0.5, 'track_popularity'] = 1
# print (df['track_popularity'].value_counts())

"""# Lyrics Data"""

final_data = pd.read_csv('/content/drive/MyDrive/final_data.csv')

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Label Encoding
label_encoder = LabelEncoder()
final_data['track_artist_encoded'] = label_encoder.fit_transform(final_data['track_artist'])
final_data['track_album_release_date_encoded'] = label_encoder.fit_transform(final_data['track_album_release_date'])
final_data['playlist_name_encoded'] = label_encoder.fit_transform(final_data['playlist_name'])

onehot_encoder_genre = OneHotEncoder(sparse=False)
onehot_encoded_genre = onehot_encoder_genre.fit_transform(final_data[['playlist_genre']])

onehot_encoder_subgenre = OneHotEncoder(sparse=False)
onehot_encoded_subgenre = onehot_encoder_subgenre.fit_transform(final_data[['playlist_subgenre']])

# Adding one-hot encoded data back to dataframe for 'playlist_genre'
for i, category in enumerate(onehot_encoder_genre.categories_[0]):
    final_data[f'playlist_genre_{category}'] = onehot_encoded_genre[:, i]

# Adding one-hot encoded data back to dataframe for 'playlist_subgenre'
for i, category in enumerate(onehot_encoder_subgenre.categories_[0]):
    final_data[f'playlist_subgenre_{category}'] = onehot_encoded_subgenre[:, i]

# print (['track_artist_encoded','track_album_release_date_encoded','playlist_name_encoded']+(final_data.columns[808:]))

df = pd.read_csv('/content/drive/MyDrive/sentence_embedding.csv')

df1 = pd.read_csv('/content/drive/MyDrive/lyrics_feature.csv')

df.drop('track_popularity_x',axis=1,inplace=True)

df = df.rename({'track_popularity_y':'track_popularity'},axis=1)

df = pd.merge(df, df1, on=['track_name','track_artist','track_popularity'], how='right')

popularity = df['track_popularity']

df.fillna(0,inplace=True)

final_data.drop_duplicates(['track_name','track_artist'],inplace=True)
final_data.drop_duplicates(['lyrics.1'],inplace=True)

final_data.index=range(len(final_data))
final_data  = final_data[['track_artist_encoded','track_album_release_date_encoded','playlist_name_encoded']+(final_data.columns[808:].tolist())]

df = pd.concat([final_data, df], axis=1)

df['track_popularity'].value_counts()

df.drop('track_popularity',axis=1,inplace=True)

df.drop(['track_artist','track_name'],axis=1,inplace=True)

# from sklearn.svm import SVC
# from sklearn.model_selection import train_test_split

# x_train, x_test, y_train, y_test = train_test_split(df[df.columns[770:]], popularity, train_size = 0.8, shuffle = True)
# scaler = StandardScaler()
# # y = popularity.to_numpy()
# # df.drop('track_popularity', axis = 1, inplace=True)
# x_train = scaler.fit_transform(x_train)
# x_test = scaler.transform(x_test)
# # y = y/100

# # Create a weighted SVM classifier
# clf = SVC(kernel='rbf', C=1, gamma=0.1)

# # Train the classifier
# clf.fit(x_train, y_train)

# # Evaluate the classifier
# accuracy = clf.score(x_test, y_test)
# print (accuracy)



"""## **bold text**#### 2d representation"""

# df.drop('track_popularity',axis=1,inplace=True)
coors = reduce_dimensionality(df, 2)

aux = pd.DataFrame({
    'title': df.track_name,
    'artist': df.track_artist,
    'popularity': popularity,
    'x': coors[:, 0],
    'y': coors[:, 1],
})

fig = px.scatter(aux, x='x', y='y', color='popularity')
fig.update_traces(marker=dict(size=5))
fig['layout']['title'] = '2D representation of the lyrics dataset'
# fig.write_image('images/lyrics_dataset2D.png', scale=3, width = 1000, height = 480)
fig.show()

"""#### 3D representation"""

coors = reduce_dimensionality(pd.concat([df[df.columns[12:24]], df[df.columns[36:804]]], axis=1), 3)

aux = pd.DataFrame({
    'title': df.track_name,
    'artist': df.track_artist,
    'popularity': df.track_popularity,
    'x': coors[:, 0],
    'y': coors[:, 1],
    'z': coors[:, 2]
})

fig = px.scatter_3d(aux, x='x', y='y', z='z', color = 'popularity')
fig.update_traces(marker=dict(size=3,
                              line=dict(width=1,
                                        color='DarkSlateGrey')),
                  selector=dict(mode='markers'))
fig['layout']['title'] = '3D representation of the lyrics dataset'
# fig.write_image('images/lyrics_dataset3D.png', scale=3, width = 1000, height = 480)
fig.show()

def lyrics_data(df,train_size, val_size):
  # Read data and drop empty target labels

  # df = df.dropna(subset=['track_popularity'])

  # Dimensionality reduction to 100 coordinates -> optimum point
  # pca = PCA(n_components=200)
  coors = df.values
  # coors = pca.fit_transform(coors)

  # Target definition
  # y = df['track_popularity'].to_numpy()
  scaler = StandardScaler()
  y = popularity.to_numpy()
  # df.drop('track_popularity', axis = 1, inplace=True)
  coors = scaler.fit_transform(coors)
  y = y/100
  # Train-Test-Val split
  x_train, x_test, y_train, y_test = train_test_split(coors, y, train_size = train_size, shuffle = True)
  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size = 1 - val_size, shuffle = True)
  return x_train, y_train, x_test, y_test, x_val, y_val

x_train, y_train, x_test, y_test, x_val, y_val = lyrics_data(df,0.8, 0.1)

x_train

"""### Model design and training"""

def generate_model():
    # model = Sequential()
    # model.add(layers.Conv1D(4, 6, padding='valid', kernel_regularizer='l1', input_shape=(819, 1)))
    # model.add(layers.MaxPooling1D(1, strides=1))
    # model.add(layers.Flatten())
    # # model.add(layers.Dense(1024, activation='relu'))
    # # model.add(layers.Dropout(0.5))  # Dropout layer to prevent overfitting
    # # model.add(layers.Dense(512, activation='relu'))
    # # model.add(layers.Dropout(0.6))  # Dropout layer to prevent overfitting
    # # model.add(layers.Dense(256, activation='relu'))
    # # model.add(layers.Dropout(0.6))  # Dropout layer to prevent overfitting
    # model.add(layers.Dense(256, activation='relu'))
    # model.add(layers.Dropout(0.6))  # Another Dropout layer
    # model.add(layers.Dense(128, activation='relu'))
    # model.add(layers.Dropout(0.6))  # Another Dropout layer
    # # model.add(layers.Dense(8, activation='relu'))
    # # # model.add(layers.Dropout(0.5))  # Another Dropout layer
    # # model.add(layers.Dense(4, activation='relu'))
    # # model.add(layers.Dropout(0.5))  # Another Dropout layer
    # model.add(layers.Dense(1, activation='sigmoid'))
    # model.compile(optimizer=Adam(learning_rate=0.0001),  # Adjusted learning rate
    #               loss='mae',
    #               metrics=['mse'])
    # return model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, Input, Reshape
from tensorflow.keras.optimizers import Adam
def generate_model():
    # Build the LSTM model
    model = Sequential()
    model.add(Input(shape=(819,)))  # Input layer
    model.add(Dense(512, activation='relu'))  # Dense layer before LSTM
    model.add(Reshape((512, 1)))  # Reshaping for LSTM
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.5))
    model.add(LSTM(32))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))  # Output layer for regression

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

    return model

model = generate_model()
plot_model(model, show_shapes=True, show_layer_activations=True)

from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

model = generate_model()
history=model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64,callbacks=[early_stopping])

# history = model.fit(
#     x_train, y_train,
#     epochs = 60, batch_size = 64,
#     validation_data=(x_val, y_val)
# )

"""Plot and save training curves"""

# Convert the history into a df to plot it
name = 'general_model'
# print (model.history)
history = history.history
df = pd.DataFrame({'mae': history['loss'],  'mse': history['mse'], 'val_mse': history['val_mse'], 'val_mae': history['val_loss']})
df.index += 1
df.index.name = 'Epochs'

# Plot the accuracy of Training and Validation
fig = px.line(df, x=df.index, y=df['mae'])
fig.add_scatter(x=df.index, y=df['val_mae'], mode='lines')
fig['data'][0].showlegend = True
fig['data'][0]['name'] = 'Training MAE'
fig['data'][1].showlegend = True
fig['data'][1]['name'] = 'Validation MAE'
fig['layout']['title'] = 'MAE evolution'
# fig.write_image('images/%s_MAE.png'%name, scale = 10, width = 1400, height = 480)
fig.show()

# Plot the loss of Training and Validation
fig = px.line(df, x=df.index, y=df['mse'])
fig.add_scatter(x=df.index, y=df['val_mse'], mode='lines')
fig['data'][0].showlegend = True
fig['data'][0]['name'] = 'Training mse'
fig['data'][1].showlegend = True#
fig['data'][1]['name'] = 'Validation mse'
fig['layout']['title'] = 'MSE evolution'
# fig.write_image('images/%s_MSE.png'%name, scale = 10, width = 1400, height = 480)
fig.show()

"""Save model"""

model.save('/content/drive/MyDrive/lyrics_model_just_lyrics.h5')

"""### Test and results analysis"""

model.evaluate(x_test, y_test)

# Predictions
y_pred = model.predict(x_test)
print(y_pred.shape, 'This y_pred shape')

# Flatten y_pred to 1D
y_pred_flat = y_pred.flatten()

# Test dataset plot preparation
coors = reduce_dimensionality(x_test, 3)

# Flatten y_test to 1D if it's 2D
y_test_flat = y_test.flatten() if y_test.ndim > 1 else y_test

print(y_test_flat.shape, 'This is y_test.shape')

# Error difference calculation
abs_difference = [abs(y_pred_flat[i] - y_test_flat[i]) for i in range(len(y_test_flat))]
diff = [y_pred_flat[i] - y_test_flat[i] for i in range(len(y_test_flat))]

# Create DataFrame
aux = pd.DataFrame({
    'x': coors[:, 0],
    'y': coors[:, 1],
    'z': coors[:, 2],
    'y_pred': y_pred_flat,
    'y_test': y_test_flat,
    'diff': diff,
    'abs_diff': abs_difference
})

aux.head()

# # Plot
# fig = px.scatter_3d(aux, x='x', y='y', z='z', color = 'abs_diff')
# fig.update_traces(marker=dict(size=3,
#                               line=dict(width=1,
#                                         color='DarkSlateGrey')),
#                   selector=dict(mode='markers'))
# fig['layout']['title'] = 'Absolute error plot of the test phase'
# fig.show()
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy import stats
from statsmodels.stats.stattools import durbin_watson

# # Assuming y_test are the true values and y_pred are the predictions from your model
# # Replace these with your actual data
# y_test = np.array([...])  # true values
# y_pred = np.array([...])  # predicted values

# Calculate residuals
residuals = y_test.flatten() - y_pred.flatten()

# Residual vs. Fitted Plot
plt.scatter(y_pred, residuals)
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.title('Residual vs. Fitted')
plt.axhline(y=0, color='grey', linestyle='dashed')
plt.show()

# Histogram of Residuals
plt.hist(residuals, bins=30)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Histogram of Residuals')
plt.show()

# Q-Q plot
sm.qqplot(residuals, line ='45')
plt.title('Normal Q-Q Plot')
plt.show()

# Durbin-Watson Test
dw_statistic = durbin_watson(residuals)
print(f'Durbin-Watson statistic: {dw_statistic}')

# Interpretation of Durbin-Watson Statistic
# Generally, a value between 1.5 and 2.5 indicates no significant autocorrelation.

# Plot
fig = px.scatter_3d(aux, x='x', y='y', z='z', color = 'diff')
fig.update_traces(marker=dict(size=3,
                              line=dict(width=1,
                                        color='DarkSlateGrey')),
                  selector=dict(mode='markers'))
fig['layout']['title'] = 'Error plot of the test phase'
fig.show()

aux['diff'].describe()

aux['diff'].plot.kde(title='Error distribution in lyrics model')

print(aux[aux['diff'] > 0.15])
print(aux[aux['diff'] < -0.15])



