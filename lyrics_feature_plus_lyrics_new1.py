# -*- coding: utf-8 -*-
"""lyrics_feature_plus_lyrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18YEm1VPkq0MBSMI4QNkS-rdfLGmNF3hN
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras import layers
from keras.optimizers import Adam, SGD
from keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import plot_model
import plotly.express as px
from sklearn.decomposition import PCA

# # For working with google drive
# from google.colab import drive
# drive.mount('/content/gdrive')

"""## Auxiliary functions"""


"""# Lyrics Data"""

final_data = pd.read_csv('final_data.csv')

# final_data.columns[808:]

# Group by 'track_artist' and calculate the mean of 'track_popularity'
artist_popularity = final_data.groupby('track_album_release_date')['track_popularity'].mean()

# Sort the artists by popularity in descending order and select the top 10
top_artists = artist_popularity.sort_values(ascending=False).head(10)

# Display the top 10 artists
print(top_artists)

# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# # Label Encoding
# label_encoder = LabelEncoder()
# # final_data['track_artist_encoded'] = label_encoder.fit_transform(final_data['track_artist'])
# final_data['track_album_release_date_encoded'] = label_encoder.fit_transform(final_data['track_album_release_date'])
# # final_data['playlist_name_encoded'] = label_encoder.fit_transform(final_data['playlist_name'])

# onehot_encoder_genre = OneHotEncoder(sparse=False)
# onehot_encoded_genre = onehot_encoder_genre.fit_transform(final_data[['playlist_genre']])

# onehot_encoder_subgenre = OneHotEncoder(sparse=False)
# onehot_encoded_subgenre = onehot_encoder_subgenre.fit_transform(final_data[['playlist_subgenre']])

# # Adding one-hot encoded data back to dataframe for 'playlist_genre'
# for i, category in enumerate(onehot_encoder_genre.categories_[0]):
#     final_data[f'playlist_genre_{category}'] = onehot_encoded_genre[:, i]

# # Adding one-hot encoded data back to dataframe for 'playlist_subgenre'
# for i, category in enumerate(onehot_encoder_subgenre.categories_[0]):
#     final_data[f'playlist_subgenre_{category}'] = onehot_encoded_subgenre[:, i]

# onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')  # handle_unknown='ignore' to deal with unseen categories

# onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
# combined_cols = final_data[['track_artist']]
# onehot_encoded = onehot_encoder.fit_transform(combined_cols)

# # Creating new column names for one-hot encoded features
# new_columns = onehot_encoder.get_feature_names_out(['track_artist'])

# # Create a new DataFrame from the one-hot encoded array
# onehot_df = pd.DataFrame(onehot_encoded, columns=new_columns)

# # If final_data has an index, align the new DataFrame's index with final_data
# onehot_df.index = final_data.index

# # Concatenate the new DataFrame with the original DataFrame
# final_data = pd.concat([final_data, onehot_df], axis=1)



# print (['track_artist_encoded','track_album_release_date_encoded','playlist_name_encoded']+(final_data.columns[808:]))

# df = pd.read_csv('/content/drive/MyDrive/sentence_embedding.csv')

# df1 = pd.read_csv('lyrics_feature.csv')

# df.drop('track_popularity_x',axis=1,inplace=True)

# df = df.rename({'track_popularity_y':'track_popularity'},axis=1)

# df = pd.merge(df, df1, on=['track_name','track_artist','track_popularity'], how='right')

# popularity = df1['track_popularity']

# df1.fillna(0,inplace=True)

final_data.drop_duplicates(['track_name','track_artist'],inplace=True)
final_data.drop_duplicates(['lyrics.1'],inplace=True)

# final_data.columns[805:].tolist()

final_data.index=range(len(final_data))
# final_data  = final_data[['track_album_release_date_encoded','lyrics.1','track_name','track_artist','track_popularity']+(final_data.columns[806:].tolist())]

# df = pd.merge(final_data, df1, on=['track_name','track_artist','track_popularity'], how='right')

# temp_df = df['track_popularity'].value_counts().reset_index()

# temp_df

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from nltk.corpus import stopwords
import re
import nltk
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras import optimizers
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Flatten
from keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras import metrics
from keras import backend as K
from keras.callbacks import EarlyStopping,ModelCheckpoint
from sklearn.metrics import classification_report,confusion_matrix
from keras.models import load_model
import tensorflow as tf
import requests
import nltk
from sklearn.preprocessing import StandardScaler
nltk.download('stopwords')
nltk.download('punkt')

stop_words = stopwords.words('english')
# stopwords_list = requests.get("https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt").content
# ext_stopwords = list(set(stopwords_list.decode().splitlines()))

# stop_words.extend(ext_stopwords)

es = EarlyStopping(monitor='val_loss',
                              min_delta=0,
                              patience=10,
                              verbose=1, mode='auto')
mc = ModelCheckpoint('covariance_final_project.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True, restore_best_weights=True)

GLOVE_6B_100D_PATH = "glove.6B.100d.txt"
encoding='utf-8'

def glove(total):
    """
    Glove word vector is read and vectors is given to each vector and dictionary
    is returned where all the words in corpus as key with their vector as value.
    Parameter:
        total(dict): dictionary containing all the words
    Return
        dictionary of words with their wordvectors
    """
    glove_small = {}
    all_words = set(w for words in total for w in words)
    with open(GLOVE_6B_100D_PATH, "rb") as infile:
        for line in infile:
            parts = line.split()
            word = parts[0].decode(encoding)
            if (word in all_words):
                nums=np.array(parts[1:], dtype=np.float32)
                glove_small[word] = nums
        infile.close()
    return glove_small

# from ftfy import fix_text
# def preprocessing(string, n=3):
#     string = fix_text(string) # fix text encoding issues
#     string = string.encode("ascii", errors="ignore").decode() #remove non ascii chars
#     string = string.lower() #make lower case
#     chars_to_remove = [")","(",".","|","[","]","{","}","'"]
#     rx = '[' + re.escape(''.join(chars_to_remove)) + ']'
#     string = re.sub(rx, '', string) #remove the list of chars defined above
#     string = string.replace('&', 'and')
#     string = string.replace(',', ' ')
#     string = string.replace('-', ' ')
#     string = string.title() # normalise case - capital at start of each word
#     string = re.sub(' +',' ',string).strip() # get rid of multiple spaces and replace with a single space
#     string = ' '+ string +' ' # pad names for ngrams...
#     string = re.sub(r'[,-./]|\sBD',r'', string)
#     ngrams = zip(*[string[i:] for i in range(n)])
#     return [''.join(ngram) for ngram in ngrams]

def preprocessing(text):
    """
    It takes the string and preprocess the string with the help pf nltk library
    Parameters:
        text(str): string which needs to be prerprocessed
    Return
        preprocessed string with no stopwords
    """
    # from nltk.corpus import stopwords
    text=str(text).lower()

    text=re.sub('[^a-z]+', ' ', text)
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]

    # remove stopwords
    # stop = stopwords.words('english')
    tokens = [token for token in tokens if token not in stop_words]

    # remove words less than three letters
    tokens = [word for word in tokens if len(word) >= 3]

    # lower capitalization
    tokens = [word.lower() for word in tokens]

    # lemmatize
#    porter = PorterStemmer()
#    tokens = [porter.stem(word) for word in tokens]
    preprocessed_text= ' '.join(tokens)
    return preprocessed_text

class MeanEmbeddingVectorizer(object):
    """
    Class simply returns the vector of word

    """
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = 100

    def fit(self, X):
        return self

    def transform(self, X):
        return np.array([([self.word2vec[w] for w in words if w in self.word2vec])
            for words in X
        ])

def list_lists(filing):
    """
    Spliting the article in words
    Parameter:
        filing(series/list): contains all the filings having 10Q/K filings with mda extracted
    Return :
        List of lists of words
    """
    from tqdm import tqdm
    test=[]
    for i in tqdm(filing):
        i=preprocessing(i)
        i=i.split()
        test.append(i)
    return test

def get_covariance_vector(article,w2v,n):
    """
    Get Covariance vectors which is calculated by transposing word vectors and calculating
    By numpy and taking upper triangular matrix as vector
    Parameter:
        article(list): list of lists of words
        w2v: object from Meanembeddingvectoriser
        n(int): number of articles
    Return
        numpy vector
    """
    covariance_vector=[]
    for i in article:
    #    i = train_article[4]
        word_vector = w2v.transform([i])
        word_vector = word_vector.reshape(word_vector.shape[1],100)
        word_vector_transpose = word_vector.transpose()
        covariance_matrix = np.cov(word_vector_transpose)
        covariance_matrix = covariance_matrix.astype(np.float16, copy=False)
        upper_tri_covar_matrix = covariance_matrix[np.triu_indices(100)]
        covariance_vector.append(upper_tri_covar_matrix)
    covariance_vector=np.array(covariance_vector,dtype='float16')
    covariance_vector=covariance_vector.reshape(n,5050,1)

    return covariance_vector

def check_model(model,x,y,x_test,y_test,es,mc):
    """
    Takes model and training set and test set and check the model by taking look on val_loss
    if it is decreasing or not
    """
    model.fit(x,y,batch_size=8,epochs=100,verbose=1,validation_data=(x_test, y_test),callbacks=[es,mc])

def build_regression_model():
    """
    Building model using deep neural network using keras for regression
    """
    model = Sequential()
    model.add(Dense(2048, activation='relu', input_shape=(5050, 1)))
    model.add(Dense(1024, activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))  # Change to linear for regression
    model.summary()
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.001)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
    model.compile(loss='mean_squared_error',  # Change to mean squared error for regression
                  optimizer=optimizer)
    return model

# final_data.drop_duplicates(['track_name','track_artist'],inplace=True)
# final_data.drop_duplicates(['lyrics.1'],inplace=True)
# df.dropna(inplace=True)
article = list_lists(final_data['lyrics.1'])
w2v_train = glove(article)
w2v=MeanEmbeddingVectorizer(w2v_train).fit(article)
X = get_covariance_vector(article,w2v,len(article))

df = pd.DataFrame(X.reshape(12170,5050))
df['track_popularity'] = final_data['track_popularity'].tolist()
df.dropna(inplace=True)

popularity = df['track_popularity'].tolist()
df.drop('track_popularity',axis=1,inplace=True)
# df.drop(['track_artist','track_name'],axis=1,inplace=True)
# df.drop('lyrics.1',axis=1,inplace=True)

# len(df)





# df['lyrics.1']

# from sklearn.svm import SVC
# from sklearn.model_selection import train_test_split

# x_train, x_test, y_train, y_test = train_test_split(df[df.columns[770:]], popularity, train_size = 0.8, shuffle = True)
# scaler = StandardScaler()
# # y = popularity.to_numpy()
# # df.drop('track_popularity', axis = 1, inplace=True)
# x_train = scaler.fit_transform(x_train)
# x_test = scaler.transform(x_test)
# # y = y/100

# # Create a weighted SVM classifier
# clf = SVC(kernel='rbf', C=1, gamma=0.1)

# # Train the classifier
# clf.fit(x_train, y_train)

# # Evaluate the classifier
# accuracy = clf.score(x_test, y_test)
# print (accuracy)



"""## **bold text**#### 2d representation"""

# # df.drop('track_popularity',axis=1,inplace=True)
# coors = reduce_dimensionality(df, 2)

# aux = pd.DataFrame({
#     'title': df.track_name,
#     'artist': df.track_artist,
#     'popularity': popularity,
#     'x': coors[:, 0],
#     'y': coors[:, 1],
# })

# fig = px.scatter(aux, x='x', y='y', color='popularity')
# fig.update_traces(marker=dict(size=5))
# fig['layout']['title'] = '2D representation of the lyrics dataset'
# # fig.write_image('images/lyrics_dataset2D.png', scale=3, width = 1000, height = 480)
# fig.show()

"""#### 3D representation"""

# coors = reduce_dimensionality(pd.concat([df[df.columns[12:24]], df[df.columns[36:804]]], axis=1), 3)

# aux = pd.DataFrame({
#     'title': df.track_name,
#     'artist': df.track_artist,
#     'popularity': df.track_popularity,
#     'x': coors[:, 0],
#     'y': coors[:, 1],
#     'z': coors[:, 2]
# })

# fig = px.scatter_3d(aux, x='x', y='y', z='z', color = 'popularity')
# fig.update_traces(marker=dict(size=3,
#                               line=dict(width=1,
#                                         color='DarkSlateGrey')),
#                   selector=dict(mode='markers'))
# fig['layout']['title'] = '3D representation of the lyrics dataset'
# # fig.write_image('images/lyrics_dataset3D.png', scale=3, width = 1000, height = 480)
# fig.show()

# df['lyrics.1']

# import numpy as  np
def lyrics_data(df,train_size, val_size):
  # Read data and drop empty target labels

  # df = df.dropna(subset=['track_popularity'])

  # Dimensionality reduction to 100 coordinates -> optimum point
  pca = PCA(n_components=200)
  coors = df.values
  coors = pca.fit_transform(coors)

  # Target definition
  # y = df['track_popularity'].to_numpy()
  # scaler = StandardScaler()
  y = np.array(popularity)
  # df.drop('track_popularity', axis = 1, inplace=True)
  # coors = scaler.fit(coors)
  # y = scaler.transform(y.reshape(-1,1))
  y = y/98
  # Train-Test-Val split
  x_train, x_test, y_train, y_test = train_test_split(coors, y, train_size = train_size, shuffle = True)
  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size = 1 - val_size, shuffle = True)
  return x_train, y_train, x_test, y_test, x_val, y_val

x_train, y_train, x_test, y_test, x_val, y_val = lyrics_data(df, 0.8, 0.1)

# (y_train).plot.kde(title='Error distribution in lyrics model')

"""### Model design and training"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, SimpleRNN, Input, Reshape
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.regularizers import l1_l2

def generate_model():
  model = Sequential()
  model.add(layers.Conv1D(6, 4, padding='valid', kernel_regularizer='l2', input_shape=(200, 1)))
  model.add(layers.MaxPooling1D(2, strides=1))
  model.add(layers.Flatten())
  # model.add(layers.Dense(512, activation='relu'))
  # model.add(Dropout(0.4))
  model.add(layers.Dense(256, activation='relu'))
  model.add(Dropout(0.3))
  model.add(layers.Dense(128, activation='relu'))
  model.add(Dropout(0.3))
  model.add(layers.Dense(100, activation ='sigmoid'))
  model.add(layers.Dense(50, activation ='sigmoid'))
  model.add(layers.Dense(1, activation ='sigmoid'))
  model.compile(optimizer=Adam(learning_rate=0.0001),
                loss= 'mse',
                metrics=['mae'])
  return model

model = generate_model()
print(model.summary())

# x_train.shape

from keras.callbacks import EarlyStopping
# model_name = 'bert-base-uncased'
# tokenizer = BertTokenizer.from_pretrained(model_name)

# # Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# # Assuming x_train, x_val are lists of text samples
# train_encodings = tokenizer(x_train, truncation=True, padding='max_length', max_length=512)
# val_encodings = tokenizer(x_val, truncation=True, padding='max_length', max_length=512)

# x_train = [np.array(train_encodings['input_ids']), np.array(train_encodings['attention_mask'])]
# x_val = [np.array(val_encodings['input_ids']), np.array(val_encodings['attention_mask'])]
# model = generate_model()
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64, callbacks=[early_stopping])

# regression_model = build_regression_model()

# # Replace 'check_model' with a function appropriate for regression
# def train_regression_model(model, x, y, x_val, y_val, es, mc):
#     model.fit(x, y, batch_size=8, epochs=100, verbose=1, validation_data=(x_val, y_val), callbacks=[es, mc])

# # Train the model
# train_regression_model(regression_model, x_train, y_train, x_val, y_val, es, mc)
# model = load_model('/content/drive/MyDrive/covariance_final_project.h5')
# Train the model
# history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64)

from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score
# Predicting values on the validation set
predictions = model.predict(x_test)

# Calculate additional metrics
rmse = mean_squared_error(y_test, predictions, squared=False)
mape = mean_absolute_percentage_error(y_test, predictions)
r_squared = r2_score(y_test, predictions)
# Number of observations and predictors
n = len(y_test)  # Number of data points in the validation set
p = x_train.shape[1]  # Total features from BERT input, attention mask, and numerical features

# Calculate Adjusted R-Squared
adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)

# Print the metrics
print(f'Validation R-Squared: {r_squared}')
print(f'Validation Adjusted R-Squared: {adjusted_r_squared}')
print(f'Validation RMSE: {rmse}')
print(f'Validation MAPE: {mape}')

# history = model.fit(
#     x_train, y_train,
#     epochs = 60, batch_size = 64,
#     validation_data=(x_val, y_val)
# )



from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score
# Predicting values on the validation set
predictions = model.predict(x_val)

# Calculate additional metrics
rmse = mean_squared_error(y_val, predictions, squared=False)
mape = mean_absolute_percentage_error(y_val, predictions)
r_squared = r2_score(y_val, predictions)
# Number of observations and predictors
n = len(y_val) # Number of data points in the validation set
p = x_train.shape[1]

# Calculate Adjusted R-Squared
adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)

# Print the metrics
print(f'Validation R-Squared: {r_squared}')
print(f'Validation Adjusted R-Squared: {adjusted_r_squared}')
print(f'Validation RMSE: {rmse}')
print(f'Validation MAPE: {mape}')

"""Plot and save training curves"""

# # Convert the history into a df to plot it
# name = 'general_model'
# # print (model.history)
# history = history.history
# df = pd.DataFrame({'train_loss': history['loss'],  'val_loss': history['val_loss'], 'val_mse': history['val_mse'], 'val_mae': history['val_loss']})
# df.index += 1
# df.index.name = 'Epochs'

# # Plot the accuracy of Training and Validation
# fig = px.line(df, x=df.index, y=df['train_loss'])
# fig.add_scatter(x=df.index, y=df['val_loss'], mode='lines')
# fig['data'][0].showlegend = True
# fig['data'][0]['name'] = 'Training MAE'
# fig['data'][1].showlegend = True
# fig['data'][1]['name'] = 'Validation MAE'
# fig['layout']['title'] = 'MAE evolution'
# # fig.write_image('images/%s_MAE.png'%name, scale = 10, width = 1400, height = 480)
# fig.show()

# # Plot the loss of Training and Validation
# fig = px.line(df, x=df.index, y=df['train_loss'])
# fig.add_scatter(x=df.index, y=df['val_loss'], mode='lines')
# fig['data'][0].showlegend = True
# fig['data'][0]['name'] = 'Training mse'
# fig['data'][1].showlegend = True#
# fig['data'][1]['name'] = 'Validation mse'
# fig['layout']['title'] = 'MSE evolution'
# # fig.write_image('images/%s_MSE.png'%name, scale = 10, width = 1400, height = 480)
# fig.show()

"""Save model"""

# model.save('/content/drive/MyDrive/lyrics_model_just_lyrics.h5')

"""### Test and results analysis"""

model.evaluate(x_test, y_test)

# Predictions
y_pred = model.predict(x_test)
print(y_pred.shape, 'This y_pred shape')

# Flatten y_pred to 1D
y_pred_flat = y_pred.flatten()

# Test dataset plot preparation
# coors = reduce_dimensionality(x_test, 3)

# Flatten y_test to 1D if it's 2D
y_test_flat = y_test.flatten() if y_test.ndim > 1 else y_test

print(y_test_flat.shape, 'This is y_test.shape')

# Error difference calculation
abs_difference = [abs(y_pred_flat[i] - y_test_flat[i]) for i in range(len(y_test_flat))]
diff = [y_pred_flat[i] - y_test_flat[i] for i in range(len(y_test_flat))]

# Create DataFrame
aux = pd.DataFrame({
    'y_pred': y_pred_flat,
    'y_test': y_test_flat,
    'diff': diff,
    'abs_diff': abs_difference
})

print (aux.loc[aux['y_test']==0,'y_pred'].describe())

# # Plot
# fig = px.scatter_3d(aux, x='x', y='y', z='z', color = 'abs_diff')
# fig.update_traces(marker=dict(size=3,
#                               line=dict(width=1,
#                                         color='DarkSlateGrey')),
#                   selector=dict(mode='markers'))
# fig['layout']['title'] = 'Absolute error plot of the test phase'
# fig.show()
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy import stats
from statsmodels.stats.stattools import durbin_watson

# # Assuming y_test are the true values and y_pred are the predictions from your model
# # Replace these with your actual data
# y_test = np.array([...])  # true values
# y_pred = np.array([...])  # predicted values

# Calculate residuals
residuals = y_test.flatten() - y_pred.flatten()

# Residual vs. Fitted Plot
plt.scatter(y_pred, residuals)
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.title('Residual vs. Fitted')
plt.axhline(y=0, color='grey', linestyle='dashed')
plt.show()

# Histogram of Residuals
plt.hist(residuals, bins=30)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Histogram of Residuals')
plt.show()

# Q-Q plot
sm.qqplot(residuals, line ='45')
plt.title('Normal Q-Q Plot')
plt.show()
plt.savefig('normal_distri.png')
# Durbin-Watson Test
dw_statistic = durbin_watson(residuals)
print(f'Durbin-Watson statistic: {dw_statistic}')

# Interpretation of Durbin-Watson Statistic
# Generally, a value between 1.5 and 2.5 indicates no significant autocorrelation.

# # Plot
# fig = px.scatter_3d(aux, x='x', y='y', z='z', color = 'diff')
# fig.update_traces(marker=dict(size=3,
#                               line=dict(width=1,
#                                         color='DarkSlateGrey')),
#                   selector=dict(mode='markers'))
# fig['layout']['title'] = 'Error plot of the test phase'
# fig.show()

aux['diff'].describe()

aux['y_pred'].describe()

aux['diff'].plot.kde(title='Error distribution in lyrics model').savefig('')

print(aux[aux['diff'] > 0.15])
print(aux[aux['diff'] < -0.15])



